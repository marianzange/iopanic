<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>iopanic</title>
    <link>http://www.iopanic.com/categories/software-engineering/index.xml</link>
    <description>Recent content on iopanic</description>
    <generator>Hugo -- gohugo.io</generator>
    <atom:link href="http://www.iopanic.com/categories/software-engineering/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Building and deploying docker images from Travis to ECR</title>
      <link>http://www.iopanic.com/post/deploying_to_ecr_from_travis/</link>
      <pubDate>Thu, 22 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.iopanic.com/post/deploying_to_ecr_from_travis/</guid>
      <description>&lt;p&gt;&lt;i&gt;This article assumes that you&amp;rsquo;re generally familiar with Docker, Amazon IAM, Travis CI and some basic bash scripting.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;I used to use Quay as a private registry but found that when heavily relying on AWS just using ECR gets the job done nicely and is a bit cheaper. However, AWS doesn&amp;rsquo;t give you a build environment liek Quay does. So you have several options: Setting up your own, pushing images from the developer machine or simply using your already existing CI infrastructure to build and push your images.&lt;/p&gt;

&lt;p&gt;For one of my recent projects, I chose to use Travis to push the images to &lt;a href=&#34;https://aws.amazon.com/ecr/&#34;&gt;ECR&lt;/a&gt;. Having a proper CI process in place ensures, that no images are pushed before they pass all the required tests. Since Travis can execute arbitrary scripts, it&amp;rsquo;s quite easy to get it to build and push your image. No need for an extra Docker build environment.&lt;/p&gt;

&lt;h2&gt;The deploy script&lt;/h2&gt;

&lt;p&gt;Below you can see the little bash script, that is executed in &lt;code&gt;after_success&lt;/code&gt; and does the magic. It&amp;rsquo;s quite easy to follow and customize to your needs, so I won&amp;rsquo;t describe it in detail. You can either place this script in your project repo or on a remote location and just curl it when you need it.&lt;/p&gt;

&lt;pre&gt;
#!/usr/bin/env bash

if ! [ $TRAVIS_PULL_REQUEST == &#34;false&#34; ]; then
  echo &#34;This is a pull request. Skipping docker build and ECR deployment.&#34;;
  exit 0;
fi

TAG=`if [ &#34;$TRAVIS_BRANCH&#34; == &#34;master&#34; ]; then echo &#34;latest&#34;; else echo $TRAVIS_BRANCH ; fi`
COMMIT=${TRAVIS_COMMIT::8}

docker --version
pip install --user awscli
export PATH=$PATH:$HOME/.local/bin
eval $(aws ecr get-login --region us-east-1)

docker build -t $DOCKER_REPO .

docker tag $DOCKER_REPO $DOCKER_ECR/$DOCKER_REPO:$TAG
docker push $DOCKER_ECR/$DOCKER_REPO:$TAG
&lt;/pre&gt;

&lt;h2&gt;Travis file&lt;/h2&gt;

&lt;p&gt;To call this script, you can simply add it to the &lt;code&gt;after_success&lt;/code&gt; section in your &lt;code&gt;travis.yml&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;
sudo: required
language: node_js
node_js:
  - &#39;6&#39;
services:
  - docker
env:
  global:
    - DOCKER_ECR=ACCOUNTID.dkr.ecr.us-east-1.amazonaws.com
    - DOCKER_REPO=REPO_NAME
after_success:
  - ./deploy_ecr
&lt;/pre&gt;

&lt;p&gt;Don&amp;rsquo;t forget to set your &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; environment variables.&lt;/p&gt;

&lt;h2&gt;IAM policy&lt;/h2&gt;

&lt;p&gt;To allow your IAM user to acquire a temporary auth token for ECR, you&amp;rsquo;ll need to attach the following policy (or alternatively more fine grained if you want).&lt;/p&gt;

&lt;pre&gt;
{
    &#34;Version&#34;: &#34;2012-10-17&#34;,
    &#34;Statement&#34;: [{
        &#34;Effect&#34;: &#34;Allow&#34;,
        &#34;Action&#34;: [
            &#34;ecr:GetAuthorizationToken&#34;
        ],
        &#34;Resource&#34;: [
            &#34;*&#34;
        ]
    }]
}
&lt;/pre&gt;

&lt;h2&gt;ECR repository permissions&lt;/h2&gt;

&lt;p&gt;And to make things even more fun, you&amp;rsquo;ll also need to set a policy on your actual ECR repo. Below you can see an example policy specifying a &amp;lsquo;WritePolicy&amp;rsquo; for the IAM user you use to push your Docker images and a &amp;lsquo;ReadPolicy&amp;rsquo; for an EC2 instance role to get images. This is just an example and you should adapt this to your own needs.&lt;/p&gt;

&lt;pre&gt;
{
    &#34;Version&#34;: &#34;2008-10-17&#34;,
    &#34;Statement&#34;: [{
        &#34;Sid&#34;: &#34;WritePolicy&#34;,
        &#34;Effect&#34;: &#34;Allow&#34;,
        &#34;Principal&#34;: {
            &#34;AWS&#34;: &#34;arn:aws:iam::123456789:user/ecr-deployer&#34;
        },
        &#34;Action&#34;: [
            &#34;ecr:DescribeRepositories&#34;,
            &#34;ecr:GetRepositoryPolicy&#34;,
            &#34;ecr:ListImages&#34;,
            &#34;ecr:DescribeImages&#34;,
            &#34;ecr:DeleteRepository&#34;,
            &#34;ecr:BatchDeleteImage&#34;,
            &#34;ecr:SetRepositoryPolicy&#34;,
            &#34;ecr:DeleteRepositoryPolicy&#34;,
            &#34;ecr:GetDownloadUrlForLayer&#34;,
            &#34;ecr:BatchGetImage&#34;,
            &#34;ecr:BatchCheckLayerAvailability&#34;,
            &#34;ecr:PutImage&#34;,
            &#34;ecr:InitiateLayerUpload&#34;,
            &#34;ecr:UploadLayerPart&#34;,
            &#34;ecr:CompleteLayerUpload&#34;
        ]
    }, {
        &#34;Sid&#34;: &#34;ReadPolicy&#34;,
        &#34;Effect&#34;: &#34;Allow&#34;,
        &#34;Principal&#34;: {
            &#34;AWS&#34;: [
                &#34;arn:aws:iam::309127738623:role/SomeServerRole&#34;
            ]
        },
        &#34;Action&#34;: [
            &#34;ecr:GetDownloadUrlForLayer&#34;,
            &#34;ecr:BatchGetImage&#34;,
            &#34;ecr:BatchCheckLayerAvailability&#34;
        ]
    }]
}
&lt;/pre&gt;

&lt;p&gt;I think it&amp;rsquo;s a fairly easy setup and if you already pay for Travis, why not use it also to build your Docker images. I hope this little guide was helpful and you can enjoy your new Travis/ECR setup.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Get your sh*t together: Docker development environments with Dusty</title>
      <link>http://www.iopanic.com/post/docker_dusty/</link>
      <pubDate>Thu, 28 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.iopanic.com/post/docker_dusty/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve seen hell, when it comes to software development and actual production environments. Inconsitent platforms, packages and a plethora of hacks to &amp;lsquo;get everything to work&amp;rsquo; on all developer machines. I&amp;rsquo;ve seen whole teams screwing with a shared &amp;lsquo;dev database&amp;rsquo; on a root server somewhere, creating a horrible mess of inconsistent behavior and ultimately slow down development and testing. This article will give you an overview how you can use Docker and Dusty to streamline your dev environments.&lt;/p&gt;

&lt;p&gt;In production, we get a similar picture in many companies. A bunch of servers, running somewhere and a whole lot of custom bash scripts doing stuff no one really wants to know. Luckily we live in a world now where everyone has heard of Docker. With docker you can containerize everything and mix and match whatever you need.&lt;/p&gt;

&lt;p&gt;The purpose of this post is not to give you an intro to Docker and Dusty, but to basically present you an example dev environment with some useful addons that help you with logging, monitoring and config management. If you don&amp;rsquo;t know about Dusty yet, read their excellent docs at &lt;a href=&#34;https://dusty.readthedocs.org&#34;&gt;https://dusty.readthedocs.org&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;developing-with-docker&#34;&gt;Developing with Docker&lt;/h2&gt;

&lt;p&gt;Developing with Docker has always been a bit tricky. Sure, there&amp;rsquo;s Docker Compose and you can mount your code from your disk into your container. But it always seemed a bit messy and since there wasn&amp;rsquo;t any clean structure around developing in containerized environments it tended to get messy again.&lt;/p&gt;

&lt;p&gt;Fortunately, the great folks at GameChanger came up with Dusty. A neat little tools that makes working with dockerized dev environments fun fun fun. Dusty allows you to configure your whole environment with yaml files (basically like docker compose) but adds useful components and structure to the whole thing. Especially mixing exactly the components you need and decide which code you want to mount into a containers is dead simple with Dusty.&lt;/p&gt;

&lt;h2 id=&#34;get-started-with-dusty&#34;&gt;Get Started with Dusty&lt;/h2&gt;

&lt;p&gt;Follow the Dusty install docs and set everything up. Once you&amp;rsquo;ve installed Dusty and all its requirements, run &lt;code&gt;$ dusty setup&lt;/code&gt; and follow the prompts. If you want to used my example repository, enter &lt;code&gt;github.com/marianzange/dusty-boilerplate&lt;/code&gt; as specs repo.&lt;/p&gt;

&lt;p&gt;If you now run &lt;code&gt;$ dusty repos list&lt;/code&gt; you should see something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+----------------------------------------------------+-------------------------+----------------+
|                     Full Name                      |        Short Name       | Local Override |
+----------------------------------------------------+-------------------------+----------------+
|      github.com/marianzange/dusty-boilerplate      |    dusty-boilerplate    |                |
| github.com/marianzange/dusty-boilerplate-flask.git | dusty-boilerplate-flask |                |
+----------------------------------------------------+-------------------------+----------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first repo contains our specs and the second repo our actual app. Now run &lt;code&gt;$ dusty bundles list&lt;/code&gt; to see the available bundles.
If you want to run everything, activate them with &lt;code&gt;$ dusty bundles activate boilerplate&lt;/code&gt; and &lt;code&gt;$ dusty bundles activate devtools&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Well, and now it&amp;rsquo;s time to fire up the whole thing! Run &lt;code&gt;$ dusty up&lt;/code&gt; and watch the magic happen. Please note, that it can take ages for the environment to start up when you run
it for the first time because it will pull all the docker images it needs. Subsequent starts will be super fast though.
If everything is working, you should see something along these lines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[...]
Compiling together the assembled specs
Compiling the port specs
Compiling the nginx config
Creating setup and script bash files
Compiling docker-compose config
Saving port forwarding to hosts file
Configuring NFS
Saving updated nginx config to the VM
Saving Docker Compose config and starting all containers
Warning: --allow-insecure-ssl is deprecated and has no effect.
It will be removed in a future version of Compose.
Creating dusty_dustyInternalNginx_1...
Creating dusty_cadvisor_1...
Creating dusty_etcd_1...
Creating dusty_etcd-browser_1...
Creating dusty_logio_1...
Creating dusty_logio-harvester_1...
Creating dusty_memcached_1...
Creating dusty_postgres_1...
Creating dusty_flask_1...
Your local environment is now started!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it&amp;rsquo;s time to access your awesome app. Open your browser and go to &lt;a href=&#34;http://flask.dusty-boilerplate.io&#34;&gt;http://flask.dusty-boilerplate.io&lt;/a&gt;.
You should see an awesome website telling you how awesome Docker and Dusty are. If you&amp;rsquo;re now wondering
why you can access your local container via dusty-boilerplate.io, it&amp;rsquo;s simple: Dusty runs and nginx
proxy that automatically proxies all your requests to the correct container. This way you can use
vanity URLs for local development. I would recommend using a standard compliant domain that&amp;rsquo;s not registered by anyone
to avoid browser autocorrect etc.&lt;/p&gt;

&lt;h2 id=&#34;specs-structure&#34;&gt;Specs Structure&lt;/h2&gt;

&lt;p&gt;To work with Dusty, you need to create a specs repo. Specs are a simple set of yml files specifying how your environment looks like. My example specs repo (&lt;a href=&#34;https://github.com/marianzange/dusty-boilerplate&#34;&gt;https://github.com/marianzange/dusty-boilerplate&lt;/a&gt;) has the following structure:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.
├── LICENSE
├── apps
│   ├── cadvisor.yml
│   ├── etcd-browser.yml
│   ├── flask.yml
│   ├── logio-harvester.yml
│   └── logio.yml
├── bundles
│   ├── boilerplate.yml
│   └── devtools.yml
├── libs
└── services
    ├── etcd.yml
    ├── memcached.yml
    ├── mysql.yml
    ├── postgres.yml
    └── solr.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the example, the app we want to work with is called &amp;lsquo;flask&amp;rsquo;. It&amp;rsquo;s a simple Flask hello world example which is hosted here: &lt;a href=&#34;https://github.com/marianzange/dusty-boilerplate-flask&#34;&gt;https://github.com/marianzange/dusty-boilerplate-flask&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;managing-configurations-with-etcd&#34;&gt;Managing Configurations with etcd&lt;/h2&gt;

&lt;p&gt;There are many different ways to manage configs. One of my favorite ways is etcd. It&amp;rsquo;s lightweight and you can use it locally and in production the same way.
Since we&amp;rsquo;re trying to get a fully isolated mix-n-match development environment, our etcd is contained in our Dusty specs as a service.&lt;/p&gt;

&lt;p&gt;Additionally we have an app called etcd-browser which allows you to populate your etcd key-value store with whatever settings you need.
Now we just need our processes to pickup and monitor relavant values from etcd. To do this, I&amp;rsquo;m using etcdenv (&lt;a href=&#34;https://github.com/upfluence/etcdenv&#34;&gt;https://github.com/upfluence/etcdenv&lt;/a&gt;)
which wraps your process and restarts it automatically if it detects changes to a specific subset of keys on etcd.&lt;/p&gt;

&lt;p&gt;If you take a look at my example &lt;code&gt;apps/flask.yml&lt;/code&gt;, you&amp;rsquo;ll find the following configuration in the &amp;lsquo;commands&amp;rsquo; section:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;commands:
  once:
    - curl -L https://github.com/upfluence/etcdenv/releases/download/v0.3.1/etcdenv-linux-amd64-0.3.1 &amp;gt; /usr/local/bin/etcdenv
    - chmod +x /usr/local/bin/etcdenv
    - pip install -r requirements.txt
  always:
    # etcdenv wraps the the executed python process and provides the environment
    # variables from etcd. In this example it looks for kv stored under
    # /apps/boilerplate/flask and will restart the py process on any detected change.
    - |
      etcdenv \
      -s http://etcd0:4001 \
      -n /apps/boilerplate/flask \
      -b restart \
      python app.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, we get a built of etcdenv and write it to the container. Since this is a specific strategy for my development
enviroment, it wouldn&amp;rsquo;t make sense to embed it in the docker app container. The main command for my container is etcdenv which wraps my app.py.
Now, whenever I change anything on etcd in the &lt;code&gt;/apps/boilerplate/flask&lt;/code&gt; subset, etcdenv will automatically restart the wrapped process.&lt;/p&gt;

&lt;p&gt;The nice parts is, you can use a fairly similar strategy to configure your production containers. So you can get pretty close to having
the same configuration flow both on production and in development.&lt;/p&gt;

&lt;p&gt;To add and change values in your etcd, my example repo comes with etcd-browser. Open &lt;a href=&#34;http://etcd.dusty-boilerplate.io&#34;&gt;http://etcd.dusty-boilerplate.io&lt;/a&gt; in your browser and enter &lt;code&gt;http://192.168.99.100:4001&lt;/code&gt; in
the connect field. Now this is not very elegant to be honest. I couldn&amp;rsquo;t find the time to improve it though, so coming up with a better way for editing your etcd data is left as an exercise to the reader.&lt;/p&gt;

&lt;p&gt;All etcd data is persisted, so no need to worry when restarting your etcd container.&lt;/p&gt;

&lt;h2 id=&#34;logging&#34;&gt;Logging&lt;/h2&gt;

&lt;p&gt;If you develop an application, you obviously want to have a realtime stream of relavant logs always open. There are multiple ways to do so with Dusty. Either your run &lt;code&gt;$ dusty logs app_name&lt;/code&gt; or you can aggregate your docker logs with log.io or another tool.&lt;/p&gt;

&lt;p&gt;My boilerplate repo contains a &amp;lsquo;logio&amp;rsquo; and a &amp;lsquo;logio-server&amp;rsquo; app. The first one is a harvester, collecting everything it gets from Docker and the second one is the actual log.io server. If your environment with these apps is up and running, simply head to &lt;a href=&#34;http://logs.dusty-boilerplate.io&#34;&gt;http://logs.dusty-boilerplate.io&lt;/a&gt; and you&amp;rsquo;ll see a beautiful real-time aggregation of logs from your containers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.iopanic.com/images/posts/docker-logio.jpg&#34; alt=&#34;log.io stream&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;monitoring&#34;&gt;Monitoring&lt;/h2&gt;

&lt;p&gt;There are many moving parts in a development environment and sometimes you&amp;rsquo;re wondering why your CPU is suddenly feeling the heat.
To get some quick insights into what going on with your containers, I&amp;rsquo;ve added Google CAdvisor (&lt;a href=&#34;https://github.com/google/cadvisor&#34;&gt;https://github.com/google/cadvisor&lt;/a&gt;) to the toolbox. CAdvisor
shows you real-time stats about your docker containers and makes it easy to identify high level problems in case a containers goes haywire.&lt;/p&gt;

&lt;p&gt;Just open up &lt;a href=&#34;http://cadvisor.dusty-boilerplate.io&#34;&gt;http://cadvisor.dusty-boilerplate.io&lt;/a&gt; and you&amp;rsquo;ll see some pretty graphs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.iopanic.com/images/posts/cadvisor.jpg&#34; alt=&#34;CAdvisor&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I hope you found this article helpful. As I&amp;rsquo;ve mentioned earlier, it was not my intention to give an intro to Docker and Dusty,
but present some opinionated aspects and additional tools that can help you when working with Docker and Dusty.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>